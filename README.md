<h2> Pre-trained Language Model Papers </h2>

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(1).pdf" style="text-decoration:none;">Semi-supervised Sequence Learning</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(2).pdf" style="text-decoration:none;">Unsupervised Pretraining for Sequence to Sequence Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(3).pdf" style="text-decoration:none;">Deep contextualized word representations</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(4).pdf" style="text-decoration:none;">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>                              




<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(5).pdf" style="text-decoration:none;">Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(6).pdf" style="text-decoration:none;">Assessing BERT's Syntactic Abilities</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(7).pdf" style="text-decoration:none;">Cross-lingual Language Model Pretraining</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(8).pdf" style="text-decoration:none;"> BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(9).pdf" style="text-decoration:none;">VideoBERT: A Joint Model for Video and Language Representation Learning</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(10).pdf" style="text-decoration:none;">75 Languages, 1 Model: Parsing Universal Dependencies Universally</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(11).pdf" style="text-decoration:none;">Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(12).pdf" style="text-decoration:none;">ERNIE: Enhanced Representation through Knowledge Integration</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(13).pdf" style="text-decoration:none;">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(14).pdf" style="text-decoration:none;">Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(15).pdf" style="text-decoration:none;">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(16).pdf" style="text-decoration:none;">What do you learn from context? Probing for sentence structure in contextualized word representations </a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(17).pdf" style="text-decoration:none;">Are Sixteen Heads Really Better than One?</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(18).pdf" style="text-decoration:none;">Defending Against Neural Fake News</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(19).pdf" style="text-decoration:none;">Open Sesame: Getting Inside BERT's Linguistic Knowledge</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(20).pdf" style="text-decoration:none;">Visualizing and Measuring the Geometry of BERT</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(21).pdf" style="text-decoration:none;">Analyzing the Structure of Attention in a Transformer Language Model</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(22).pdf" style="text-decoration:none;">What Does BERT Look At?
An Analysis of BERT's Attention</a></li> 
 
 
 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(23).pdf" style="text-decoration:none;">Learning Video Representations using Contrastive Bidirectional Transformer</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(24).pdf" style="text-decoration:none;">Pre-Training with WholeWord Masking for Chinese BERT</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(25).pdf" style="text-decoration:none;">SpanBERT: Improving Pre-training by Representing and Predicting Spans</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(26).pdf" style="text-decoration:none;">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></li>
 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(27).pdf" style="text-decoration:none;">Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(28).pdf" style="text-decoration:none;">ERNIE 2.0: A Continual Pre-training Framework for Language Understanding</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(29).pdf" style="text-decoration:none;">What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(30).pdf" style="text-decoration:none;">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(31).pdf" style="text-decoration:none;">VisualBERT: A Simple and Performant Baseline for Vision and Language</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(32).pdf" style="text-decoration:none;">On Identifiability in Transformers</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(33).pdf" style="text-decoration:none;">Fusion of Detected Objects in Text for Visual Question Answering</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(34).pdf" style="text-decoration:none;">Visualizing and Understanding the Effectiveness of BERT</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(35).pdf" style="text-decoration:none;">Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(36).pdf" style="text-decoration:none;">Universal Adversarial Triggers for Attacking and Analyzing NLP</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(37).pdf" style="text-decoration:none;">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(38).pdf" style="text-decoration:none;">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(39).pdf" style="text-decoration:none;">Revealing the Dark Secrets of BERT</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(40).pdf" style="text-decoration:none;">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(41).pdf" style="text-decoration:none;">Patient Knowledge Distillation for BERT Model Compression</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(42).pdf" style="text-decoration:none;">Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(43).pdf" style="text-decoration:none;">Small and Practical BERT Models for Sequence Labeling</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(44).pdf" style="text-decoration:none;">How Contextual are ContextualizedWord Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(45).pdf" style="text-decoration:none;">Language Models as Knowledge Bases?</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(46).pdf" style="text-decoration:none;">The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(47).pdf" style="text-decoration:none;">Investigating BERT's Knowledge of Language: Five Analysis Methods with NPIs</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(48).pdf" style="text-decoration:none;">Knowledge Enhanced ContextualWord Representations</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(49).pdf" style="text-decoration:none;">MultiFiT: Efficient Multi-lingual Language Model Fine-tuning</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(50).pdf" style="text-decoration:none;">How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(51).pdf" style="text-decoration:none;">Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(52).pdf" style="text-decoration:none;">K-BERT: Enabling Language Representation with Knowledge Graph</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(53).pdf" style="text-decoration:none;">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(54).pdf" style="text-decoration:none;">Extreme Language Model Compression with Optimal Subwords and Shared Projections </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(55).pdf" style="text-decoration:none;">UNITER: UNiversal Image-TExt Representation Learning</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(56).pdf" style="text-decoration:none;">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(57).pdf" style="text-decoration:none;">Transformers: State-of-the-Art Natural Language Processing</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(58).pdf" style="text-decoration:none;">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(59).pdf" style="text-decoration:none;">On the Cross-lingual Transferability of Monolingual Representations</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(60).pdf" style="text-decoration:none;">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(61).pdf" style="text-decoration:none;">ExpBERT: Representation Engineering with Natural Language Explanations</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(62).pdf" style="text-decoration:none;">SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(63).pdf" style="text-decoration:none;">DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(64).pdf" style="text-decoration:none;">Do You Have the Right Scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(65).pdf" style="text-decoration:none;">BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(66).pdf" style="text-decoration:none;">Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(67).pdf" style="text-decoration:none;">TAPAS: Weakly Supervised Table Parsing via Pre-training</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(68).pdf" style="text-decoration:none;">Finding Universal Grammatical Relations in Multilingual BERT</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(69).pdf" style="text-decoration:none;">Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(70).pdf" style="text-decoration:none;">Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(71).pdf" style="text-decoration:none;">TABERT: Pretraining for Joint Understanding of Textual and Tabular Data</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(72).pdf" style="text-decoration:none;">A Mutual Information Maximization Perspective of Language Representation Learning</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(73).pdf" style="text-decoration:none;">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations </a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(74).pdf" style="text-decoration:none;">BERT is Not an Interlingua and the Bias of Tokenization</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(75).pdf" style="text-decoration:none;">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(76).pdf" style="text-decoration:none;">FreeLB: Enhanced Adversarial Training for Natural Language Understanding</a></li>


   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(77).pdf" style="text-decoration:none;">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(78).pdf" style="text-decoration:none;">Language Models are Unsupervised Multitask Learners</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(79).pdf" style="text-decoration:none;">Improving Language Understanding by Generative Pre-Training</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(80).pdf" style="text-decoration:none;">Multilingual Alignment of Contextual Word Representations</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(81).pdf" style="text-decoration:none;">Linguistic Knowledge and Transferability of Contextual Representations </a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(82).pdf" style="text-decoration:none;">A Structural Probe for Finding Syntax in Word Representations</a></li> 
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(83).pdf" style="text-decoration:none;">Universal Language Model Fine-tuning for Text Classification</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(84).pdf" style="text-decoration:none;">ERNIE: Enhanced Language Representation with Informative Entities</a></li> 
 
  
   <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(85).pdf" style="text-decoration:none;">What does BERT learn about the structure of language?</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(86).pdf" style="text-decoration:none;">Multi-Task Deep Neural Networks for Natural Language Understanding</a></li> 
  
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(87).pdf" style="text-decoration:none;">Probing Neural Network Comprehension of Natural Language Arguments</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(88).pdf" style="text-decoration:none;">How multilingual is Multilingual BERT?</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(89).pdf" style="text-decoration:none;">Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(90).pdf" style="text-decoration:none;">Reducing Transformer Depth on Demand with Structured Dropout</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(91).pdf" style="text-decoration:none;">StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(92).pdf" style="text-decoration:none;">Thieves on Sesame Street! Model Extraction of BERT-based APIs</a></li>


<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(93).pdf" style="text-decoration:none;">To Tune or Not to Tune?
Adapting Pretrained Representations to Diverse Tasks</a></li>
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(94).pdf" style="text-decoration:none;">PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation</a></li> 
 
 
 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(95).pdf" style="text-decoration:none;">Unified Language Model Pre-training for Natural Language Understanding and Generation</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(96).pdf" style="text-decoration:none;">Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(97).pdf" style="text-decoration:none;">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(98).pdf" style="text-decoration:none;">TinyBERT: Distilling BERT for Natural Language Understanding</a></li>


 <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(99).pdf" style="text-decoration:none;">How Language-Neutral is Multilingual BERT?</a></li>
  <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(100).pdf" style="text-decoration:none;">Cross-Lingual Ability of Multilingual BERT: An Empirical Study</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(101).pdf" style="text-decoration:none;">BERT Rediscovers the Classical NLP Pipeline</a></li>                        
<li><a target="_blank" href="https://github.com/manjunath5496/Pre-trained-Language-Model-Papers/blob/master/plm(102).pdf" style="text-decoration:none;">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a></li>






</ul>
